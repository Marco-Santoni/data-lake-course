{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Iceberg is an open table format for huge analytic datasets. It is designed to improve on the performance and usability of existing table formats like Hive, Hudi, and Delta Lake.\n",
    "\n",
    "PyIceberg is a Python library for interacting with Apache Iceberg tables.\n",
    "\n",
    "In this tutorial, we will understand the metadata files of Apache Iceberg using PyIceberg and local files only.\n",
    "\n",
    "```bash\n",
    "conda install pyiceberg\n",
    "conda install sqlalchemy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "table_dir = \"iceberg_warehouse\"\n",
    "\n",
    "if os.path.exists(table_dir):\n",
    "    shutil.rmtree(table_dir)\n",
    "\n",
    "os.makedirs(table_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration, we'll configure the catalog to use the SqlCatalog implementation, which will store information in a local sqlite database. We'll also configure the catalog to store data files in the local filesystem instead of an object store. This should not be used in production due to the limited scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "\n",
    "warehouse_path = os.path.abspath(\"./iceberg_warehouse\")\n",
    "catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n",
    "        \"warehouse\": f\"file://{warehouse_path}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside this SQLite database?\n",
    "\n",
    "This database is the actual catalog that lists the Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the SQLite database and look what is inside ðŸ‘€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyiceberg.catalog import Catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import NestedField, StringType, LongType\n",
    "from pyiceberg.table import Table\n",
    "from pyiceberg.io import FileIO\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Step 2: Create a local directory for the Iceberg table\n",
    "\n",
    "table_dir = \"iceberg_table\"\n",
    "os.makedirs(table_dir, exist_ok=True)\n",
    "\n",
    "# Step 3: Initialize an Iceberg table\n",
    "\n",
    "# Define the schema for the table\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"id\", field_type=LongType(), required=False),\n",
    "    NestedField(field_id=1, name=\"name\", field_type=StringType(), required=False)\n",
    ")\n",
    "\n",
    "# Create a catalog and table\n",
    "table = catalog.create_table(\"default.my_table\", schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the SQLite catalog that there is a reference to a JSON metadata file. Let's open it.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write data to table. PyIceberg is nicely integrated with PyArrow. We create an Arrow table and append it to the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "id: int64\n",
       "name: string\n",
       "----\n",
       "id: [[1,2]]\n",
       "name: [[\"Alice\",\"Bob\"]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "# Step 4: Add some data to the table\n",
    "\n",
    "# Define some data\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\"}\n",
    "]\n",
    "# Create a PyArrow Table from the list of dictionaries\n",
    "arrow_table = pa.Table.from_pylist(data)\n",
    "\n",
    "# Write the data to the table\n",
    "arrow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Support for parsing catalog level identifier in Catalog identifiers is deprecated. Please refer to the table using only its namespace and its table name.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n"
     ]
    }
   ],
   "source": [
    "table.append(arrow_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/avro/decoder.py:185: UserWarning: Falling back to pure Python Avro decoder, missing Cython implementation\n",
      "  warnings.warn(\"Falling back to pure Python Avro decoder, missing Cython implementation\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "id: int64\n",
       "name: large_string\n",
       "----\n",
       "id: [[1,2]]\n",
       "name: [[\"Alice\",\"Bob\"]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.scan().to_arrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look again at the catalog database. The `metadata_location` has changed. It is now pointing to a new JSON file. We can now look at it. We can see that the `snapshots` list has a record. That record is now referring to a `manifest-list`.\n",
    "\n",
    "Now, let's jump back to our slides ðŸ›‘\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now open the `manifest-list`file. It is actually an Avro file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manifest_path': 'file:///Users/marcosantoni/Desktop/data-lake-course/local_pyiceberg/iceberg_warehouse/default.db/my_table/metadata/01d98fe8-70df-420d-bdb4-b06812f13d9d-m0.avro', 'manifest_length': 4367, 'partition_spec_id': 0, 'content': 0, 'sequence_number': 1, 'min_sequence_number': 1, 'added_snapshot_id': 4588393757938124859, 'added_files_count': 1, 'existing_files_count': 0, 'deleted_files_count': 0, 'added_rows_count': 2, 'existing_rows_count': 0, 'deleted_rows_count': 0, 'partitions': [], 'key_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "metadata_folder = './iceberg_warehouse/default.db/my_table/metadata'\n",
    "\n",
    "reader = DataFileReader(open(os.path.join(metadata_folder, 'snap-4588393757938124859-0-01d98fe8-70df-420d-bdb4-b06812f13d9d.avro'), \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    # a generator to loop over dictionaries\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then look at the actual manifest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 1, 'snapshot_id': 4588393757938124859, 'sequence_number': None, 'file_sequence_number': None, 'data_file': {'content': 0, 'file_path': 'file:///Users/marcosantoni/Desktop/data-lake-course/local_pyiceberg/iceberg_warehouse/default.db/my_table/data/00000-0-01d98fe8-70df-420d-bdb4-b06812f13d9d.parquet', 'file_format': 'PARQUET', 'partition': {}, 'record_count': 2, 'file_size_in_bytes': 915, 'column_sizes': [{'key': 1, 'value': 118}, {'key': 2, 'value': 90}], 'value_counts': [{'key': 1, 'value': 2}, {'key': 2, 'value': 2}], 'null_value_counts': [{'key': 1, 'value': 0}, {'key': 2, 'value': 0}], 'nan_value_counts': [], 'lower_bounds': [{'key': 1, 'value': b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'}, {'key': 2, 'value': b'Alice'}], 'upper_bounds': [{'key': 1, 'value': b'\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00'}, {'key': 2, 'value': b'Bob'}], 'key_metadata': None, 'split_offsets': [4], 'equality_ids': None, 'sort_order_id': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/avro/schema.py:1233: IgnoredLogicalType: Unknown map, using array.\n",
      "  warnings.warn(avro.errors.IgnoredLogicalType(f\"Unknown {logical_type}, using {type_}.\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "metadata_folder = './iceberg_warehouse/default.db/my_table/metadata'\n",
    "\n",
    "reader = DataFileReader(open(os.path.join(metadata_folder, '01d98fe8-70df-420d-bdb4-b06812f13d9d-m0.avro'), \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    # a generator to loop over dictionaries\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is actually inside the Parquet file? It contains the actual data content of the Iceberg table (that's why it is actually in the `data` folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "id: int64\n",
       "name: string\n",
       "----\n",
       "id: [[1,2]]\n",
       "name: [[\"Alice\",\"Bob\"]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = '00000-0-01d98fe8-70df-420d-bdb4-b06812f13d9d.parquet'\n",
    "table_dir = 'iceberg_warehouse/default.db/my_table/data'\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table_from_parquet = pq.read_table(os.path.join(table_dir, parquet_file))\n",
    "table_from_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Support for parsing catalog level identifier in Catalog identifiers is deprecated. Please refer to the table using only its namespace and its table name.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n"
     ]
    }
   ],
   "source": [
    "# Define some data\n",
    "data = [\n",
    "    {\"id\": 3, \"name\": \"Daniel\"}\n",
    "]\n",
    "# Create a PyArrow Table from the list of dictionaries\n",
    "arrow_table = pa.Table.from_pylist(data)\n",
    "table.append(arrow_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is now in the SQLite catalog? The `metadata_location` now has a new path. Let's open that JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the latest `manifest-list`. The list now contains 2 `manifest-path` records. We see that the first one has `added_rows_count` equals to `2` (the first insert operation) while the second one has `added_rows_count` equals to `1` (last insert operation). Let's open the last manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manifest_path': 'file:///Users/marcosantoni/Desktop/data-lake-course/local_pyiceberg/iceberg_warehouse/default.db/my_table/metadata/4ebc7486-8441-4f91-8b59-6857b02be11c-m0.avro', 'manifest_length': 4372, 'partition_spec_id': 0, 'content': 0, 'sequence_number': 2, 'min_sequence_number': 2, 'added_snapshot_id': 5542359964143702113, 'added_files_count': 1, 'existing_files_count': 0, 'deleted_files_count': 0, 'added_rows_count': 1, 'existing_rows_count': 0, 'deleted_rows_count': 0, 'partitions': [], 'key_metadata': None}\n",
      "{'manifest_path': 'file:///Users/marcosantoni/Desktop/data-lake-course/local_pyiceberg/iceberg_warehouse/default.db/my_table/metadata/c8943001-21fe-4384-aadc-9c505d248da3-m0.avro', 'manifest_length': 4368, 'partition_spec_id': 0, 'content': 0, 'sequence_number': 1, 'min_sequence_number': 1, 'added_snapshot_id': 4869232518670708754, 'added_files_count': 1, 'existing_files_count': 0, 'deleted_files_count': 0, 'added_rows_count': 2, 'existing_rows_count': 0, 'deleted_rows_count': 0, 'partitions': [], 'key_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "metadata_folder = './iceberg_warehouse/default.db/my_table/metadata'\n",
    "\n",
    "reader = DataFileReader(open(os.path.join(metadata_folder, 'snap-5542359964143702113-0-4ebc7486-8441-4f91-8b59-6857b02be11c.avro'), \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    # a generator to loop over dictionaries\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the manifest file. Thanks to metadata like for example `upper_bounds` compute engines can exploit for faster queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 1, 'snapshot_id': 5542359964143702113, 'sequence_number': None, 'file_sequence_number': None, 'data_file': {'content': 0, 'file_path': 'file:///Users/marcosantoni/Desktop/data-lake-course/local_pyiceberg/iceberg_warehouse/default.db/my_table/data/00000-0-4ebc7486-8441-4f91-8b59-6857b02be11c.parquet', 'file_format': 'PARQUET', 'partition': {}, 'record_count': 1, 'file_size_in_bytes': 909, 'column_sizes': [{'key': 1, 'value': 110}, {'key': 2, 'value': 88}], 'value_counts': [{'key': 1, 'value': 1}, {'key': 2, 'value': 1}], 'null_value_counts': [{'key': 1, 'value': 0}, {'key': 2, 'value': 0}], 'nan_value_counts': [], 'lower_bounds': [{'key': 1, 'value': b'\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00'}, {'key': 2, 'value': b'Daniel'}], 'upper_bounds': [{'key': 1, 'value': b'\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00'}, {'key': 2, 'value': b'Daniel'}], 'key_metadata': None, 'split_offsets': [4], 'equality_ids': None, 'sort_order_id': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcosantoni/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/avro/schema.py:1233: IgnoredLogicalType: Unknown map, using array.\n",
      "  warnings.warn(avro.errors.IgnoredLogicalType(f\"Unknown {logical_type}, using {type_}.\"))\n"
     ]
    }
   ],
   "source": [
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "metadata_folder = './iceberg_warehouse/default.db/my_table/metadata'\n",
    "\n",
    "reader = DataFileReader(open(os.path.join(metadata_folder, '4ebc7486-8441-4f91-8b59-6857b02be11c-m0.avro'), \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    # a generator to loop over dictionaries\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's open the Datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "id: int64\n",
       "name: string\n",
       "----\n",
       "id: [[3]]\n",
       "name: [[\"Daniel\"]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = '00000-0-4ebc7486-8441-4f91-8b59-6857b02be11c.parquet'\n",
    "table_dir = 'iceberg_warehouse/default.db/my_table/data'\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table_from_parquet = pq.read_table(os.path.join(table_dir, parquet_file))\n",
    "table_from_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each operation creates a new snapshot of the table and a new datafile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
