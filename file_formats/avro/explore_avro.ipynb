{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://avro.apache.org/docs/1.11.1/getting-started-python/\n",
    "\n",
    "Setup\n",
    "\n",
    "```bash\n",
    "conda create -n data_file_formats python=3.12\n",
    "conda activate data_file_formats\n",
    "conda install -c conda-forge notebook\n",
    "pip install avro=1.12.0\n",
    "pip install fastavro=1.9.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Avro Files\n",
    "\n",
    "Avro is a row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. Avro is designed to support data-intensive applications and is widely used in big data processing frameworks like Apache Hadoop and Apache Spark.\n",
    "\n",
    "Key features of Avro:\n",
    "- Compact, fast, binary data format.\n",
    "- Rich data structures.\n",
    "- A container file, to store persistent data.\n",
    "- Simple integration with dynamic languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a schema\n",
    "\n",
    "Avro schemas are defined using JSON. Schemas are composed of primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed).\n",
    "\n",
    "We added an example in `user.avsc`file (AVro SChema).\n",
    "\n",
    "We also define a namespace (`“namespace”: “example.avro”`), which together with the name attribute defines the “full name” of the schema (`example.avro.User` in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize our first file\n",
    "\n",
    "Data in Avro is always stored with its corresponding schema, meaning we can always read a serialized item, regardless of whether we know the schema ahead of time. This allows us to perform serialization and deserialization without code generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alyssa', 'favorite_number': 256, 'favorite_color': None}\n",
      "{'name': 'Ben', 'favorite_number': 7, 'favorite_color': 'red'}\n"
     ]
    }
   ],
   "source": [
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "\n",
    "schema = avro.schema.parse(open(\"user.avsc\", \"rb\").read())\n",
    "\n",
    "writer = DataFileWriter(open(\"users.avro\", \"wb\"), DatumWriter(), schema)\n",
    "writer.append({\"name\": \"Alyssa\", \"favorite_number\": 256})\n",
    "writer.append({\"name\": \"Ben\", \"favorite_number\": 7, \"favorite_color\": \"red\"})\n",
    "writer.close()\n",
    "\n",
    "reader = DataFileReader(open(\"users.avro\", \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to write data with an improper schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AvroTypeException",
     "evalue": "The datum \"{'name': 'Alyssa', 'favorite_number': 256, 'favorite_food': 'pizza'}\" provided for \"User\" is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"namespace\": \"example.avro\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"name\"\n    },\n    {\n      \"type\": [\n        \"int\",\n        \"null\"\n      ],\n      \"name\": \"favorite_number\"\n    },\n    {\n      \"type\": [\n        \"string\",\n        \"null\"\n      ],\n      \"name\": \"favorite_color\"\n    }\n  ]\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAvroTypeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m avro\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser.avsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      3\u001b[0m writer2 \u001b[38;5;241m=\u001b[39m DataFileWriter(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers2.avro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m), DatumWriter(), schema)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwriter2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlyssa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfavorite_number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfavorite_food\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpizza\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will raise an error\u001b[39;00m\n\u001b[1;32m      5\u001b[0m writer2\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/avro/datafile.py:259\u001b[0m, in \u001b[0;36mDataFileWriter.append\u001b[0;34m(self, datum)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, datum: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Append a datum to the file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatum_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# if the data to write is larger than the sync interval, write the block\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/avro/io.py:1002\u001b[0m, in \u001b[0;36mDatumWriter.write\u001b[0;34m(self, datum, encoder)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriters_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m avro\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mIONotReadyException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot write without a writer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1002\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriters_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriters_schema, datum, encoder)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_file_formats/lib/python3.12/site-packages/avro/io.py:137\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(expected_schema, datum, raise_on_error)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m avro\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mAvroTypeException(current_node\u001b[38;5;241m.\u001b[39mschema, current_node\u001b[38;5;241m.\u001b[39mname, current_node\u001b[38;5;241m.\u001b[39mdatum)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# preserve the prior validation behavior of returning false when there are problems.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# if there are children of this node to append, do so.\u001b[39;00m\n",
      "\u001b[0;31mAvroTypeException\u001b[0m: The datum \"{'name': 'Alyssa', 'favorite_number': 256, 'favorite_food': 'pizza'}\" provided for \"User\" is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"namespace\": \"example.avro\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"name\"\n    },\n    {\n      \"type\": [\n        \"int\",\n        \"null\"\n      ],\n      \"name\": \"favorite_number\"\n    },\n    {\n      \"type\": [\n        \"string\",\n        \"null\"\n      ],\n      \"name\": \"favorite_color\"\n    }\n  ]\n}"
     ]
    }
   ],
   "source": [
    "schema = avro.schema.parse(open(\"user.avsc\", \"rb\").read())\n",
    "\n",
    "writer2 = DataFileWriter(open(\"users2.avro\", \"wb\"), DatumWriter(), schema)\n",
    "writer2.append({\"name\": \"Alyssa\", \"favorite_number\": 256, 'favorite_food': 'pizza'})  # This will raise an error\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alyssa', 'favorite_number': 256, 'favorite_color': None}\n",
      "{'name': 'Ben', 'favorite_number': 7, 'favorite_color': 'red'}\n"
     ]
    }
   ],
   "source": [
    "reader = DataFileReader(open(\"users.avro\", \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    # a generator to loop over dictionaries\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Avro files\n",
    "\n",
    "Often, Avro files can be captured by daily or streaming jobs. We now write the multiple Avro files with the same schema in the same folder. We then see how to read all those files in a single Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "if os.path.exists('users_split'):\n",
    "    shutil.rmtree('users_split')\n",
    "os.makedirs('users_split', exist_ok=True)\n",
    "\n",
    "# Define two lists of users\n",
    "users_list1 = [\n",
    "    {\"name\": \"Alyssa\", \"favorite_number\": 256},\n",
    "    {\"name\": \"Ben\", \"favorite_number\": 7, \"favorite_color\": \"red\"}\n",
    "]\n",
    "\n",
    "users_list2 = [\n",
    "    {\"name\": \"Charlie\", \"favorite_number\": 42},\n",
    "    {\"name\": \"Diana\", \"favorite_number\": 99, \"favorite_color\": \"blue\"}\n",
    "]\n",
    "\n",
    "# Write the first list of users to an Avro file\n",
    "with DataFileWriter(open(\"users_split/users_list1.avro\", \"wb\"), DatumWriter(), schema) as writer:\n",
    "    for user in users_list1:\n",
    "        writer.append(user)\n",
    "\n",
    "# Write the second list of users to another Avro file\n",
    "with DataFileWriter(open(\"users_split/users_list2.avro\", \"wb\"), DatumWriter(), schema) as writer:\n",
    "    for user in users_list2:\n",
    "        writer.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastavro\n",
    "import os\n",
    "\n",
    "# Function to read avro files and return a list of records\n",
    "def read_avro(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        return [record for record in reader]\n",
    "\n",
    "# Directory containing the avro files\n",
    "directory = 'users_split'\n",
    "\n",
    "# List to hold all records\n",
    "all_records = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.avro'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        all_records.extend(read_avro(file_path))\n",
    "\n",
    "# Create a pandas DataFrame from the list of records\n",
    "df = pd.DataFrame(all_records)\n",
    "print(df)u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a Python script that reads all the Avro files in the `users_split` subfolder and deserializes them into a single Pandas DataFrame. The script should iterate over all files in the specified directory, read each Avro file, and collect all records into a list. Finally, convert this list of records into a Pandas DataFrame and print the DataFrame. Use the `fastavro` library for reading the Avro files. The resulting DataFrame should contain all the records from the Avro files in the `users_split` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  favorite_number favorite_color\n",
      "0  Charlie               42           None\n",
      "1    Diana               99           blue\n",
      "2   Alyssa              256           None\n",
      "3      Ben                7            red\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fastavro\n",
    "import os\n",
    "\n",
    "# Function to read avro files and return a list of records\n",
    "def read_avro(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        return [record for record in reader]\n",
    "\n",
    "# Directory containing the avro files\n",
    "directory = 'users_split'\n",
    "\n",
    "# List to hold all records\n",
    "all_records = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.avro'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        all_records.extend(read_avro(file_path))\n",
    "\n",
    "# Create a pandas DataFrame from the list of records\n",
    "df = pd.DataFrame(all_records)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
