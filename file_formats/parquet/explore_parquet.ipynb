{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parquet file\n",
    "\n",
    "How can we create a Parquet file in Python?\n",
    "\n",
    "Let's start from a Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'one': [-1, 0, 2.5],\n",
    "        'two': ['foo', 'bar', 'baz'],\n",
    "        'three': [True, False, True]\n",
    "    },\n",
    "    index=list('abc')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Apache Arrow _specification_.\n",
    "\n",
    "\n",
    "> Apache Arrow was born from the need for a **set of standards** around tabular data representation and interchange between systems. The adoption of these standards reduces computing costs of data serialization/deserialization and implementation costs across systems implemented in different programming languages.\n",
    "\n",
    "In Python, we can use PyArrow, the Python implementation of the Arrow specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pyarrow.Table object\n",
    "\"\"\"The PyArrow Table type is not part of the Apache Arrow specification, but is rather a tool to help with wrangling multiple record batches and array pieces as a single logical dataset. As a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\"\"\"\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table\n",
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet metadata..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x12f8c4220>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 4\n",
       "  num_rows: 3\n",
       "  num_row_groups: 1\n",
       "  format_version: 2.6\n",
       "  serialized_size: 2572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata('example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is metadata?\n",
    "https://parquet.apache.org/docs/file-format/metadata/\n",
    "We have just read the `FileMetadata`\n",
    "\n",
    "> The file metadata is described by the `FileMetaData` structure. This file metadata provides offset and size information useful when navigating the Parquet file. \n",
    "\n",
    "![Parquet Metadata](docs/FileFormat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger file\n",
    "\n",
    "https://www.stats.govt.nz/large-datasets/csv-files-for-download/\n",
    "https://www.stats.govt.nz/assets/Uploads/New-Zealand-business-demography-statistics/New-Zealand-business-demography-statistics-At-February-2024/Download-data/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anzsic06     Area  year  geo_count  ec_count\n",
      "0        A  A100100  2024         87       200\n",
      "1        A  A100200  2024        135       210\n",
      "2        A  A100301  2024          6        35\n",
      "3        A  A100400  2024         54        35\n",
      "4        A  A100500  2024         51        95\n",
      "(6751326, 5)\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.head())\n",
    "print(df_large.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write again this table to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(df_large), 'example_large.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig more into the Metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x13500f6f0>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 6751326\n",
       "  num_row_groups: 6752\n",
       "  format_version: 2.6\n",
       "  serialized_size: 3260180"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Parquet 100% columnar?\n",
    "\n",
    "What are row groups?\n",
    "Ref:\n",
    "https://blog.det.life/i-spent-8-hours-learning-parquet-heres-what-i-discovered-97add13fb28f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional row-wise formats store data as records, one after another, much like a database table. This format is intuitive and works well when accessing entire records frequently. However, it can be inefficient when dealing with analytics, where you often only need specific columns from a large dataset.\n",
    "\n",
    "![row storage](./docs/row-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar formats address this issue by storing data in columns instead of rows. This means that when you need specific columns, you can read only the data you need, significantly reducing the amount of data scanned.\n",
    "\n",
    "![columnar storage](./docs/columnar-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, simply storing data in a columnar format has some downsides. The record **write or update** operation requires touching **multiple column segments**, resulting in numerous **I/O** operations. This can significantly slow the write performance, especially when dealing with large datasets.\n",
    "\n",
    "In addition, when queries involve multiple columns, the database system must reconstruct the records from separate columns. The cost of this reconstruction increases with the **number of columns** involved in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![row groups](./docs/row-groups.webp)\n",
    "\n",
    "The format groups data into “row groups,” each containing a subset of rows. (horizontal partition.) Within each row group, data for each column is called a “column chunk.” (vertical partition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page header\n",
    "\n",
    "We will not cover a further level of granular metadata: `PageHeader`\n",
    "\n",
    "The page header metadata is stored with the page data and includes information such as value encoding, definition encoding, and repetition encoding. In addition to the data values, Parquet also stores definition and repetition levels to handle nested data. The application uses the page header to **read and decode** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.RowGroupMetaData object at 0x135078630>\n",
       "  num_columns: 5\n",
       "  num_rows: 1000\n",
       "  total_byte_size: 15617\n",
       "  sorting_columns: ()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's look att Row Group metadata\n",
    "\n",
    "parquet_file.metadata.row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ColumnChunkMetaData object at 0x1135e53f0>\n",
       "  file_offset: 0\n",
       "  file_path: \n",
       "  physical_type: INT64\n",
       "  num_values: 1000\n",
       "  path_in_schema: year\n",
       "  is_stats_set: True\n",
       "  statistics:\n",
       "    <pyarrow._parquet.Statistics object at 0x13500f790>\n",
       "      has_min_max: True\n",
       "      min: 2024\n",
       "      max: 2024\n",
       "      null_count: 0\n",
       "      distinct_count: None\n",
       "      num_values: 1000\n",
       "      physical_type: INT64\n",
       "      logical_type: None\n",
       "      converted_type (legacy): NONE\n",
       "  compression: SNAPPY\n",
       "  encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
       "  has_dictionary_page: True\n",
       "  dictionary_page_offset: 5607\n",
       "  data_page_offset: 5631\n",
       "  total_compressed_size: 99\n",
       "  total_uncompressed_size: 95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.metadata.row_group(0).column(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this statistic useful?\n",
    "Let's try a basic count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anzsic06     6751326\n",
      "Area         6751326\n",
      "year         6751326\n",
      "geo_count    6751326\n",
      "ec_count     6751326\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 6751326\n"
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "row_count = parquet_file.metadata.num_rows\n",
    "print(f\"Total number of rows: {row_count}\")\n",
    "#given this pyarrow parquet file, compute the count of rows in the year column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something more sophisticated, count the rows per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2000    247192\n",
      "2001    245753\n",
      "2002    246695\n",
      "2003    250130\n",
      "2004    259121\n",
      "2005    263223\n",
      "2006    265095\n",
      "2007    266518\n",
      "2008    267211\n",
      "2009    268547\n",
      "2010    267549\n",
      "2011    267954\n",
      "2012    268210\n",
      "2013    267911\n",
      "2014    270514\n",
      "2015    273210\n",
      "2016    275177\n",
      "2017    277515\n",
      "2018    278822\n",
      "2019    281313\n",
      "2020    282621\n",
      "2021    283621\n",
      "2022    290242\n",
      "2023    293312\n",
      "2024    293870\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "count_per_year = df_large.groupby('year').size()\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we avoid using Pandas, but just PyArrow engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "year: int64\n",
      "year_count: int64\n",
      "----\n",
      "year: [[2024,2020,2019,2015,2014,...,2022,2007,2012,2021,2002]]\n",
      "year_count: [[293870,282621,281313,273210,270514,...,290242,266518,268210,283621,246695]]\n"
     ]
    }
   ],
   "source": [
    "tbl = pq.read_table('example_large.parquet')\n",
    "count_per_year = tbl.group_by('year').aggregate([('year', 'count')])\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we been lucky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.8749954289989545 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "    count_per_year = df_large.groupby('year').size()\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.04289828349428717 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    tbl = pq.read_table('example_large.parquet')\n",
    "    count_per_year = tbl.group_by('year').aggregate([('year', 'count')])\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks promising. We understood a bit more about\n",
    "- the metadata of a Parquet file\n",
    "- how data is stored in row groups and column chunks\n",
    "- why is this relevant for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned the power of metadata, but is this metadata making the overall file larger?\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 18.72 MB\n",
      "File size: 139.39 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_size = os.path.getsize('example_large.parquet') / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.2f} MB\")\n",
    "\n",
    "\n",
    "file_size = os.path.getsize('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv') / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is actually an order-of-magnitude smaller. That's good news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "Next, let's use more features of writing Parquet files. The improvements we obtained so far were low-hanging fruits. We now loot at how partitioning works.\n",
    "\n",
    "We need to use the [write_dataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.write_dataset.html#pyarrow.dataset.write_dataset) function of PyArrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "arrow_table = pa.Table.from_pandas(df_large)\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "ds.write_dataset(\n",
    "    arrow_table,\n",
    "    \"./example_large_partitioned\",\n",
    "    format='parquet',\n",
    "    partitioning=ds.partitioning(pa.schema([(\"year\", pa.int32())]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2000\u001b[m\u001b[m\n",
      "\u001b[34m2001\u001b[m\u001b[m\n",
      "\u001b[34m2002\u001b[m\u001b[m\n",
      "\u001b[34m2003\u001b[m\u001b[m\n",
      "\u001b[34m2004\u001b[m\u001b[m\n",
      "\u001b[34m2005\u001b[m\u001b[m\n",
      "\u001b[34m2006\u001b[m\u001b[m\n",
      "\u001b[34m2007\u001b[m\u001b[m\n",
      "\u001b[34m2008\u001b[m\u001b[m\n",
      "\u001b[34m2009\u001b[m\u001b[m\n",
      "\u001b[34m2010\u001b[m\u001b[m\n",
      "\u001b[34m2011\u001b[m\u001b[m\n",
      "\u001b[34m2012\u001b[m\u001b[m\n",
      "\u001b[34m2013\u001b[m\u001b[m\n",
      "\u001b[34m2014\u001b[m\u001b[m\n",
      "\u001b[34m2015\u001b[m\u001b[m\n",
      "\u001b[34m2016\u001b[m\u001b[m\n",
      "\u001b[34m2017\u001b[m\u001b[m\n",
      "\u001b[34m2018\u001b[m\u001b[m\n",
      "\u001b[34m2019\u001b[m\u001b[m\n",
      "\u001b[34m2020\u001b[m\u001b[m\n",
      "\u001b[34m2021\u001b[m\u001b[m\n",
      "\u001b[34m2022\u001b[m\u001b[m\n",
      "\u001b[34m2023\u001b[m\u001b[m\n",
      "\u001b[34m2024\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls example_large_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-0.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls example_large_partitioned/2000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the [pyarrow.dataset.dataset()](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.dataset.html#pyarrow.dataset.dataset) function provides an interface to discover and read all those files as a single big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./example_large_partitioned/2000/part-0.parquet',\n",
       " './example_large_partitioned/2001/part-0.parquet',\n",
       " './example_large_partitioned/2002/part-0.parquet',\n",
       " './example_large_partitioned/2003/part-0.parquet',\n",
       " './example_large_partitioned/2004/part-0.parquet',\n",
       " './example_large_partitioned/2005/part-0.parquet',\n",
       " './example_large_partitioned/2006/part-0.parquet',\n",
       " './example_large_partitioned/2007/part-0.parquet',\n",
       " './example_large_partitioned/2008/part-0.parquet',\n",
       " './example_large_partitioned/2009/part-0.parquet',\n",
       " './example_large_partitioned/2010/part-0.parquet',\n",
       " './example_large_partitioned/2011/part-0.parquet',\n",
       " './example_large_partitioned/2012/part-0.parquet',\n",
       " './example_large_partitioned/2013/part-0.parquet',\n",
       " './example_large_partitioned/2014/part-0.parquet',\n",
       " './example_large_partitioned/2015/part-0.parquet',\n",
       " './example_large_partitioned/2016/part-0.parquet',\n",
       " './example_large_partitioned/2017/part-0.parquet',\n",
       " './example_large_partitioned/2018/part-0.parquet',\n",
       " './example_large_partitioned/2019/part-0.parquet',\n",
       " './example_large_partitioned/2020/part-0.parquet',\n",
       " './example_large_partitioned/2021/part-0.parquet',\n",
       " './example_large_partitioned/2022/part-0.parquet',\n",
       " './example_large_partitioned/2023/part-0.parquet',\n",
       " './example_large_partitioned/2024/part-0.parquet']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\"./example_large_partitioned\", format=\"parquet\", partitioning=ds.partitioning(pa.schema([(\"year\", pa.int32())])))\n",
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data has not been read yet. The whole dataset can be viewed as a single big table using [pyarrow.dataset.Dataset.to_table()](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html#pyarrow.dataset.Dataset.to_table).\n",
    "\n",
    "Notice that converting to a table will force all data to be loaded in memory. For big datasets is usually not what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "anzsic06: string\n",
      "Area: string\n",
      "geo_count: int64\n",
      "ec_count: int64\n",
      "year: int32\n",
      "----\n",
      "anzsic06: [[\"C25\",\"C25\",\"C25\",\"C25\",\"C25\",...,\"F341\",\"F341\",\"F341\",\"F341\",\"F341\"],[\"M70\",\"M70\",\"M70\",\"M70\",\"M70\",...,\"Q851\",\"Q851\",\"Q851\",\"Q851\",\"Q851\"],...,[\"O77\",\"O77\",\"O77\",\"O77\",\"O77\",...,\"R\",\"R\",\"R\",\"R\",\"R\"],[\"F373\",\"F373\",\"F373\",\"F373\",\"F373\",...,\"H440\",\"H440\",\"H440\",\"H440\",\"H440\"]]\n",
      "Area: [[\"A215000\",\"A215101\",\"A215200\",\"A215401\",\"A215600\",...,\"A119000\",\"A119500\",\"A119700\",\"A120000\",\"A120500\"],[\"A166400\",\"A166500\",\"A166700\",\"A167000\",\"A167101\",...,\"A146000\",\"A146400\",\"A146501\",\"A146600\",\"A146700\"],...,[\"A237100\",\"A237200\",\"A237500\",\"A237600\",\"A237700\",...,\"A161400\",\"A161500\",\"A161700\",\"A161800\",\"A161900\"],[\"R07\",\"R08\",\"R09\",\"R12\",\"R13\",...,\"A340700\",\"A341101\",\"A341201\",\"A341300\",\"A341400\"]]\n",
      "geo_count: [[0,0,0,3,3,...,0,0,0,0,3],[3,0,0,3,0,...,0,6,3,0,12],...,[0,0,3,0,3,...,15,6,3,6,0],[48,84,282,15,474,...,3,18,3,3,6]]\n",
      "ec_count: [[0,0,0,3,0,...,0,3,0,0,0],[15,0,0,0,3,...,0,6,0,0,18],...,[12,0,6,0,0,...,100,18,0,3,0],[200,230,930,9,1700,...,30,30,35,35,21]]\n",
      "year: [[2000,2000,2000,2000,2000,...,2000,2000,2000,2000,2000],[2000,2000,2000,2000,2000,...,2000,2000,2000,2000,2000],...,[2024,2024,2024,2024,2024,...,2024,2024,2024,2024,2024],[2024,2024,2024,2024,2024,...,2024,2024,2024,2024,2024]]\n"
     ]
    }
   ],
   "source": [
    "table = dataset.to_table()\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to exploit partitioning, the easiest is filtering when reading.\n",
    "\n",
    "Scan will return only the rows matching the filter. If possible the predicate will be pushed down to exploit the partition information or internal metadata found in the data source, e.g. Parquet statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "anzsic06: string\n",
      "Area: string\n",
      "geo_count: int64\n",
      "ec_count: int64\n",
      "year: int32\n",
      "----\n",
      "anzsic06: [[\"Total\",\"Total\",\"Total\",\"Total\",\"Total\",...,\"Total\",\"Total\",\"Total\",\"Total\",\"Total\"],[\"B08\",\"B08\",\"B08\",\"B08\",\"B08\",...,\"C259\",\"C259\",\"C259\",\"C259\",\"C259\"],...,[\"N73\",\"N73\",\"N73\",\"N73\",\"N73\",...,\"Q860\",\"Q860\",\"Q860\",\"Q860\",\"Q860\"],[\"Q860\",\"Q860\",\"Q860\",\"Q860\",\"Q860\",...,\"Total\",\"Total\",\"Total\",\"Total\",\"Total\"]]\n",
      "Area: [[\"A149600\",\"A149701\",\"A149702\",\"A149800\",\"A149901\",...,\"T073\",\"T074\",\"T075\",\"T076\",\"TTotal\"],[\"A335301\",\"A335701\",\"A336800\",\"A340700\",\"A343600\",...,\"A113300\",\"A113402\",\"A113800\",\"A114000\",\"A114500\"],...,[\"A330600\",\"A330700\",\"A330800\",\"A330900\",\"A331000\",...,\"A305600\",\"A305800\",\"A305900\",\"A306100\",\"A306501\"],[\"A306801\",\"A307301\",\"A307501\",\"A307601\",\"A307801\",...,\"A149100\",\"A149200\",\"A149300\",\"A149400\",\"A149500\"]]\n",
      "geo_count: [[153,270,114,477,450,...,6960,2049,5316,207252,596973],[3,0,0,0,3,...,0,3,6,0,6],...,[3,3,3,3,6,...,0,3,3,0,0],[3,0,6,3,0,...,168,300,210,111,81]]\n",
      "ec_count: [[2200,390,80,1350,330,...,16500,6600,28000,813900,2332600],[9,0,0,0,560,...,0,0,3,0,6],...,[0,12,3,6,0,...,40,220,310,0,18],[18,0,210,70,30,...,510,400,570,290,190]]\n",
      "year: [[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],...,[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020]]\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(ds.field('year') == 2020)\n",
    "table = filtered_dataset.to_table()\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Part 1 \n",
    "\n",
    "1. Download the School Dataset [CSV file](https://www.kaggle.com/datasets/abhishekbagwan/school-dataset) at Kaggle Datasets.\n",
    "2. Write the content of the CSV file as a Parquet file `school_dataset.parquet`.\n",
    "3. Then, print the content of the `FileMetadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0x123e9f740>\n",
      "  created_by: parquet-cpp-arrow version 18.0.0\n",
      "  num_columns: 17\n",
      "  num_rows: 464\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 9584\n"
     ]
    }
   ],
   "source": [
    "# Download the School Dataset CSV file\n",
    "df_school = pd.read_csv('DYCD_after-school_programs__Neighborhood_Development_Area__NDA__Family_Support.csv')\n",
    "\n",
    "# Convert the DataFrame to a PyArrow Table\n",
    "arrow_table_school = pa.Table.from_pandas(df_school)\n",
    "\n",
    "# Write the table to a Parquet file\n",
    "pq.write_table(arrow_table_school, 'school_dataset.parquet')\n",
    "\n",
    "# Read and print the FileMetadata\n",
    "metadata = pq.read_metadata('school_dataset.parquet')\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "1. Download the School Dataset [CSV file](https://www.kaggle.com/datasets/abhishekbagwan/school-dataset) at Kaggle Datasets.\n",
    "2. Write the content of the CSV file as a Parquet dataset partitioned by the column `PROGRAM` in a folder called `dycd_after_school_programs`.\n",
    "3. Read the dataset again in PyArrow. Print few lines of the dataset.\n",
    "4. Is it possible to read the `FileMetadata` of the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"Adolescent Literacy\",\n",
      "    \" High-School Aged Youth\",\n",
      "    \" Housing\",\n",
      "    \"Legal Services\",\n",
      "    \"Legal Services\",\n",
      "    ...\n",
      "    \"Family Literacy\",\n",
      "    \"Adolescent Literacy\",\n",
      "    \" Adult Literacy\",\n",
      "    \"Seniors\",\n",
      "    \" Housing\"\n",
      "  ]\n",
      "]\n",
      "pyarrow.Table\n",
      "PROGRAM TYPE: string\n",
      "SITE NAME: string\n",
      "BOROUGH / COMMUNITY: string\n",
      "AGENCY: string\n",
      "Contact Number: string\n",
      "Grade Level / Age Group : string\n",
      "Location 1: string\n",
      "Postcode: double\n",
      "Latitude: double\n",
      "Longitude: double\n",
      "Community Board: double\n",
      "Community Council : double\n",
      "Census Tract: double\n",
      "BIN: double\n",
      "BBL: double\n",
      "NTA: string\n",
      "PROGRAM: string\n",
      "----\n",
      "PROGRAM TYPE: [[\"Reading & Writing,NDA Programs,English Language Program,Family Literacy,Adult Basic Education,ABE/GED\",\"Reading & Writing,NDA Programs,English Language Program,ESOL,Family Literacy\",\"Reading & Writing,NDA Programs,English Language Program,ESOL,Family Literacy\",\"Reading & Writing,NDA Programs,English Language Program,ESOL,Family Literacy\",\"Reading & Writing,NDA Programs,English Language Program,ESOL,Family Literacy\"]]\n",
      "SITE NAME: [[\"New Heights Neighborhood Center\",\"Jewish Community Council of Greater Coney Island, Inc.\",\"Latin American Integration Center\",\"Lutheran Family Health Centers Adult & Family Education\",\"Harlem YMCA\"]]\n",
      "BOROUGH / COMMUNITY: [[\"New York\",\"Brooklyn\",\"Woodside\",\"Brooklyn\",\"New York\"]]\n",
      "AGENCY: [[\"Northern Manhattan Improvement Corporation\",\"Jewish Community Council of Greater Coney Island, Inc.\",\"Make the Road New York\",\"Lutheran Family Health Centers Adult & Family Education\",\"YMCA of Greater New York / ELESAIR\"]]\n",
      "Contact Number: [[\"(212) 822-8300\",\"(718) 449-5000 x 2237\",\"(718) 418-7690 x 211\",\"(718) 630-7150 x 4070\",\"(212) 875-4336\"]]\n",
      "Grade Level / Age Group : [[\"16+\",\"16+\",\"16+\",\"16+\",\"16+\"]]\n",
      "Location 1: [[\"216 Ft Washington Avenue\n",
      "New York, NEW YORK 10032\n",
      "(40.842343769667, -73.942200758685)\",\"1081 Coney Island Avenue\n",
      "Brooklyn, NEW YORK 11230\n",
      "(40.631085769962, -73.966320469543)\",\"46 09 Skillman Avenue\n",
      "Woodside, NEW YORK 11377\n",
      "(40.745193764481, -73.90484714408)\",\"330 59th Street\n",
      "Brooklyn, NEW YORK 11220\n",
      "(40.642201614799, -74.019175801725)\",\"180 135th Street\n",
      "New York, NEW YORK 10030\n",
      "(40.815023338273, -73.943122982412)\"]]\n",
      "Postcode: [[10032,11230,11104,11220,null]]\n",
      "Latitude: [[40.842571,40.631068,40.746551,40.642121,null]]\n",
      "Longitude: [[-73.942097,-73.966441,-73.91782,-74.018997,null]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"./dycd_after_school_programs\"):\n",
    "    shutil.rmtree(\"./dycd_after_school_programs\")\n",
    "\n",
    "df = pd.read_csv('DYCD_after-school_programs__Neighborhood_Development_Area__NDA__Family_Support.csv')\n",
    "arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "ds.write_dataset(\n",
    "    arrow_table,\n",
    "    \"./dycd_after_school_programs\",\n",
    "    format='parquet',\n",
    "    partitioning=ds.partitioning(pa.schema([(\"PROGRAM\", pa.string())]))\n",
    ")\n",
    "\n",
    "ds_2 = ds.dataset(\"./dycd_after_school_programs\", format=\"parquet\", partitioning=ds.partitioning(pa.schema([(\"PROGRAM\", pa.string())])))\n",
    "table_2 = ds_2.to_table()\n",
    "print(table_2.slice(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "1. Slice the School Dataset CSV file into two parts (200 rows the first and 264 the second).\n",
    "2. Write each part as a separate Parquet file in the same folder called `dycd_after_school_programs_split`.\n",
    "3. Read the two Parquet files into a single PyArrow table using the Datasets function.\n",
    "4. Print the combined table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "PROGRAM TYPE: string\n",
      "PROGRAM: string\n",
      "SITE NAME: string\n",
      "BOROUGH / COMMUNITY: string\n",
      "AGENCY: string\n",
      "Contact Number: string\n",
      "Grade Level / Age Group : string\n",
      "Location 1: string\n",
      "Postcode: double\n",
      "Latitude: double\n",
      "Longitude: double\n",
      "Community Board: double\n",
      "Community Council : double\n",
      "Census Tract: double\n",
      "BIN: double\n",
      "BBL: double\n",
      "NTA: string\n",
      "----\n",
      "PROGRAM TYPE: [[\"Reading & Writing,NDA Programs,Family Literacy\",\"After-School Programs,NDA Programs,Youth Educational Support\",\"Family Support,NDA Programs\",\"Immigration Services,Immigrant Support Services\",\"Immigration Services,Immigrant Support Services\",...,\"Family Support,NDA Programs\",\"Reading & Writing,NDA Programs,Family Literacy\",\"Reading & Writing,NDA Programs,English Language Program,Family Literacy,Adult Basic Education,ABE/GED\",\"Immigration Services,Immigrant Support Services\",\"Reading & Writing,NDA Programs,English Language Program,ESOL,Family Literacy\"],[\"NDA Programs,Senior Programs,Older Adult Program\",\"Family Support,NDA Programs\",\"Family Support,NDA Programs\",\"Family Support,NDA Programs\",\"After-School Programs,NDA Programs,Adolescent Literacy,Youth Educational Support\",...,\"Reading & Writing,NDA Programs,Family Literacy\",\"Reading & Writing,NDA Programs,Family Literacy\",\"Reading & Writing,NDA Programs,English Language Program,Family Literacy,Adult Basic Education,ABE/GED\",\"NDA Programs,Senior Programs,Older Adult Program\",\"Family Support,NDA Programs\"]]\n",
      "PROGRAM: [[\"Adolescent Literacy\",\" High-School Aged Youth\",\" Housing\",\"Legal Services\",\"Legal Services\",...,\"Healthy Families\",\"Family Literacy\",\" Adult Literacy\",\"Legal Assistance Program\",\" Adult Literacy\"],[\"Seniors\",\"Fatherhood Initiative\",\"Health Stat\",\" Housing\",\" Middle School Youth\",...,\"Family Literacy\",\"Adolescent Literacy\",\" Adult Literacy\",\"Seniors\",\" Housing\"]]\n",
      "SITE NAME: [[\"K 533- School for Democracy and Leadership 600 Kingston Avenue\",\"Voyagees Prepatory High School\",\"AIDS Center of Queens County Jamaica Site\",\"Sanctuary for Families, Inc. (Manhattan) \",\"Family Justice Centers in Queens and Brooklyn \",...,\"Central Brooklyn Economic Development Corporation\",\"Southeast Bronx Neighborhood Centers, Inc.\",\"The Jewish Community Center of Staten Island\",\"Family Justice Center\",\"Agudath Israel of America Community Services, Inc.\"],[\"Neighborhood SHOPP Casa Boricua Senior Center\",\"Oberia Dempsey Center \",\"Make the Road New York\",\"Shorefront Jewish Community Council\",\"MOTT HALL IV (K522)\",...,\"Edith and Carl Marks Jewish Community House of Bensonhurst\",\"JHS 22- Jordan L Mott School\",\"Washington Houses Community Center\",\"Highbridge Gardens\",\"Harlem Justice Center\"]]\n",
      "BOROUGH / COMMUNITY: [[\"Brooklyn\",\"Queens\",\"Queens\",\"Manhattan,Bronx,Queens,Staten Island, Brooklyn\",\"Manhattan,Bronx,Queens,Staten Island, Brooklyn\",...,\"Brooklyn\",\"Bronx\",\"Staten Island\",\"Brooklyn\",\"New York\"],[\"Bronx\",\"New York \",\"Staten Island\",\"Brooklyn\",\"Brooklyn\",...,\"Brooklyn\",\"Bronx\",\"Manhattan\",\"Bronx\",\"New York\"]]\n",
      "AGENCY: [[\"CAMBA\",\"Central Brooklyn Economic Development Corporation\",\"St. Luke A.M.E Church\",\"Sanctuary for Families, Inc.\",\"Sanctuary for Families, Inc.\",...,\"Northern Manhattan Improvement Corporation\",\"Southeast Bronx Neighborhood Center, Inc. (SEBNC)\",\"The Jewish Community Center of Staten Island\",\"Sanctuary for Families\",\"Agudath Israel of America Community Services, Inc.\"],[\"Bronx Family Center\",\"The Northern Manhattan Perinatal Partnership Inc.\",\"Make the Road New York\",\"Flatbush Development Corporation\",\"IS 52\",...,\"Edith and Carl Marks Jewish Community House of Bensonhurst\",\"Supportive Childrens Advocacy Network (SCAN)\",\"Union Settlement Association\",\"Children's Aid Society/East Harlem Center\",\"The Urban Assembly School for Law and Justice\"]]\n",
      "Contact Number: [[\"718.282.5575\",\"(718) 592-5757\",\"(718) 896-2500\",\"212.349.6009           ext. 264\",\"212.349.6009           ext. 264\",...,\"(718) 498-4513\",\"718-542-2727\",\"(718) 508-3881\",\"212-349-6009\",\"(212) 809-5935 x 104\"],[\"(718) 542-0006\",\"212.665.2600\",\"(347) 547-5133\",\"(718) 743-0575\",\"(212) 652-2812\",...,\"718-943-6325\",\"212.683.2522\",\"(212) 828-6298\",\"(718) 542-0006\",\"(212) 473-8090\"]]\n",
      "Grade Level / Age Group : [[\"grades 6 to 8\",\" High School\",\"Adults\",\"All Ages\",\"All Ages\",...,null,\"A parent 16 Years Old or Older \",\"16+\",\"All Ages\",\"16+\"],[\"Seniors\",\"16 to 24\",\"All Ages\",\"Adults\",\"Middle School\",...,\"A parent 16 Years Old or Older \",\"grades 6 to 8\",\"16+\",\"Seniors\",\"Adults\"]]\n",
      "Location 1: [[null,null,null,null,null,...,\"444 Thomas\n",
      "Brooklyn, NEW YORK 11212\n",
      "(40.662797169953, -73.912984894337)\",\"955 Tinton Avenue\n",
      "Bronx, NEW YORK 10456\n",
      "(40.823097254176, -73.903034140456)\",\"1297 Arthur Kill Road Staten Island\n",
      "NEW YORK 10312\n",
      "(40.565263465624, -74.184003351052)\",\"350 Jay Street\n",
      "Brooklyn, NEW YORK 11201\n",
      "(40.69351957616, -73.987264420018)\",\"225 Broadway\n",
      "New York, NEW YORK 10007\n",
      "(40.711843149995, -74.008228982076)\"],[\"910 172nd Street\n",
      "Bronx, NEW YORK 10460\n",
      "(40.834189221174, -73.89020092304)\",\"127 127th\n",
      "New York, NEW YORK 10027\n",
      "(40.809438819236, -73.945341926389)\",\"479 Port Richmond Ave Staten Island\n",
      "NEW YORK 10302\n",
      "(40.630934529097, -74.139014649681)\",\"3049 Brighton 6th Street\n",
      "Brooklyn, NEW YORK 11235\n",
      "(40.578431288323, -73.961743706863)\",\"1137 HERKIMER STREET\n",
      "Brooklyn, NEW YORK 11233\n",
      "(40.677823649732, -73.915831667237)\",...,\"7802 Bay Parkway\n",
      "Brooklyn, NEW YORK 11214\n",
      "(40.606424326706, -73.989192742721)\",\"270 167th Street\n",
      "Bronx, NEW YORK 10456\n",
      "(40.833227654258, -73.914085372255)\",\"1775 3rd Avenue\n",
      "Manhattan, NEW YORK 10029\n",
      "(40.787046638823, -73.947929348156)\",\"1145 University Avenue\n",
      "Bronx, NEW YORK 10452\n",
      "(40.83751165367, -73.927895360689)\",\"170 121st Street\n",
      "New York, NEW YORK 10035\n",
      "(40.801297147584, -73.938148706659)\"]]\n",
      "Postcode: [[null,null,null,null,null,...,null,10456,10312,11201,10007],[null,null,10302,11235,11233,...,11214,null,10029,10452,null]]\n",
      "Latitude: [[null,null,null,null,null,...,null,40.823096,40.565262,40.693524,40.71187],[null,null,40.630818,40.578435,40.677805,...,40.606369,null,40.786574,40.837077,null]]\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464, 17)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Slice the School Dataset CSV file into two parts (200 rows the first and 264 the second)\n",
    "df_school = pd.read_csv('DYCD_after-school_programs__Neighborhood_Development_Area__NDA__Family_Support.csv')\n",
    "df_part1 = df_school.iloc[:200]\n",
    "df_part2 = df_school.iloc[200:]\n",
    "\n",
    "# Write each part as a separate Parquet file in the same folder called `dycd_after_school_programs_split`\n",
    "if not os.path.exists(\"./dycd_after_school_programs_split\"):\n",
    "    os.makedirs(\"./dycd_after_school_programs_split\")\n",
    "\n",
    "table_part1 = pa.Table.from_pandas(df_part1)\n",
    "table_part2 = pa.Table.from_pandas(df_part2)\n",
    "\n",
    "pq.write_table(table_part1, './dycd_after_school_programs_split/part1.parquet')\n",
    "pq.write_table(table_part2, './dycd_after_school_programs_split/part2.parquet')\n",
    "\n",
    "# Read the two Parquet files into a single PyArrow table using the Datasets function\n",
    "dataset_split = ds.dataset(\"./dycd_after_school_programs_split\", format=\"parquet\")\n",
    "combined_table = dataset_split.to_table()\n",
    "\n",
    "# Print the combined table\n",
    "print(combined_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
