{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parquet file\n",
    "\n",
    "How can we create a Parquet file in Python?\n",
    "\n",
    "Let's start from a Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'one': [-1, 0, 2.5],\n",
    "        'two': ['foo', 'bar', 'baz'],\n",
    "        'three': [True, False, True]\n",
    "    },\n",
    "    index=list('abc')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Apache Arrow _specification_.\n",
    "\n",
    "\n",
    "> Apache Arrow was born from the need for a **set of standards** around tabular data representation and interchange between systems. The adoption of these standards reduces computing costs of data serialization/deserialization and implementation costs across systems implemented in different programming languages.\n",
    "\n",
    "In Python, we can use PyArrow, the Python implementation of the Arrow specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pyarrow.Table object\n",
    "\"\"\"The PyArrow Table type is not part of the Apache Arrow specification, but is rather a tool to help with wrangling multiple record batches and array pieces as a single logical dataset. As a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\"\"\"\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table\n",
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet metadata..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x12f8c4220>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 4\n",
       "  num_rows: 3\n",
       "  num_row_groups: 1\n",
       "  format_version: 2.6\n",
       "  serialized_size: 2572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata('example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is metadata?\n",
    "https://parquet.apache.org/docs/file-format/metadata/\n",
    "We have just read the `FileMetadata`\n",
    "\n",
    "> The file metadata is described by the `FileMetaData` structure. This file metadata provides offset and size information useful when navigating the Parquet file. \n",
    "\n",
    "![Parquet Metadata](docs/FileFormat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger file\n",
    "\n",
    "https://www.stats.govt.nz/large-datasets/csv-files-for-download/\n",
    "https://www.stats.govt.nz/assets/Uploads/New-Zealand-business-demography-statistics/New-Zealand-business-demography-statistics-At-February-2024/Download-data/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anzsic06     Area  year  geo_count  ec_count\n",
      "0        A  A100100  2024         87       200\n",
      "1        A  A100200  2024        135       210\n",
      "2        A  A100301  2024          6        35\n",
      "3        A  A100400  2024         54        35\n",
      "4        A  A100500  2024         51        95\n",
      "(6751326, 5)\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.head())\n",
    "print(df_large.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write again this table to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(df_large), 'example_large.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig more into the Metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x13500f6f0>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 6751326\n",
       "  num_row_groups: 6752\n",
       "  format_version: 2.6\n",
       "  serialized_size: 3260180"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Parquet 100% columnar?\n",
    "\n",
    "What are row groups?\n",
    "Ref:\n",
    "https://blog.det.life/i-spent-8-hours-learning-parquet-heres-what-i-discovered-97add13fb28f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional row-wise formats store data as records, one after another, much like a database table. This format is intuitive and works well when accessing entire records frequently. However, it can be inefficient when dealing with analytics, where you often only need specific columns from a large dataset.\n",
    "\n",
    "![row storage](./docs/row-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar formats address this issue by storing data in columns instead of rows. This means that when you need specific columns, you can read only the data you need, significantly reducing the amount of data scanned.\n",
    "\n",
    "![columnar storage](./docs/columnar-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, simply storing data in a columnar format has some downsides. The record **write or update** operation requires touching **multiple column segments**, resulting in numerous **I/O** operations. This can significantly slow the write performance, especially when dealing with large datasets.\n",
    "\n",
    "In addition, when queries involve multiple columns, the database system must reconstruct the records from separate columns. The cost of this reconstruction increases with the **number of columns** involved in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![row groups](./docs/row-groups.webp)\n",
    "\n",
    "The format groups data into “row groups,” each containing a subset of rows. (horizontal partition.) Within each row group, data for each column is called a “column chunk.” (vertical partition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page header\n",
    "\n",
    "We will not cover a further level of granular metadata: `PageHeader`\n",
    "\n",
    "The page header metadata is stored with the page data and includes information such as value encoding, definition encoding, and repetition encoding. In addition to the data values, Parquet also stores definition and repetition levels to handle nested data. The application uses the page header to **read and decode** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.RowGroupMetaData object at 0x135078630>\n",
       "  num_columns: 5\n",
       "  num_rows: 1000\n",
       "  total_byte_size: 15617\n",
       "  sorting_columns: ()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's look att Row Group metadata\n",
    "\n",
    "parquet_file.metadata.row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ColumnChunkMetaData object at 0x1135e53f0>\n",
       "  file_offset: 0\n",
       "  file_path: \n",
       "  physical_type: INT64\n",
       "  num_values: 1000\n",
       "  path_in_schema: year\n",
       "  is_stats_set: True\n",
       "  statistics:\n",
       "    <pyarrow._parquet.Statistics object at 0x13500f790>\n",
       "      has_min_max: True\n",
       "      min: 2024\n",
       "      max: 2024\n",
       "      null_count: 0\n",
       "      distinct_count: None\n",
       "      num_values: 1000\n",
       "      physical_type: INT64\n",
       "      logical_type: None\n",
       "      converted_type (legacy): NONE\n",
       "  compression: SNAPPY\n",
       "  encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
       "  has_dictionary_page: True\n",
       "  dictionary_page_offset: 5607\n",
       "  data_page_offset: 5631\n",
       "  total_compressed_size: 99\n",
       "  total_uncompressed_size: 95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.metadata.row_group(0).column(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this statistic useful?\n",
    "Let's try a basic count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anzsic06     6751326\n",
      "Area         6751326\n",
      "year         6751326\n",
      "geo_count    6751326\n",
      "ec_count     6751326\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 6751326\n"
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "row_count = parquet_file.metadata.num_rows\n",
    "print(f\"Total number of rows: {row_count}\")\n",
    "#given this pyarrow parquet file, compute the count of rows in the year column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something more sophisticated, count the rows per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2000    247192\n",
      "2001    245753\n",
      "2002    246695\n",
      "2003    250130\n",
      "2004    259121\n",
      "2005    263223\n",
      "2006    265095\n",
      "2007    266518\n",
      "2008    267211\n",
      "2009    268547\n",
      "2010    267549\n",
      "2011    267954\n",
      "2012    268210\n",
      "2013    267911\n",
      "2014    270514\n",
      "2015    273210\n",
      "2016    275177\n",
      "2017    277515\n",
      "2018    278822\n",
      "2019    281313\n",
      "2020    282621\n",
      "2021    283621\n",
      "2022    290242\n",
      "2023    293312\n",
      "2024    293870\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "count_per_year = df_large.groupby('year').size()\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we avoid using Pandas, but just PyArrow engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "year: int64\n",
      "year_count: int64\n",
      "----\n",
      "year: [[2024,2020,2019,2015,2014,...,2022,2007,2012,2021,2002]]\n",
      "year_count: [[293870,282621,281313,273210,270514,...,290242,266518,268210,283621,246695]]\n"
     ]
    }
   ],
   "source": [
    "tbl = pq.read_table('example_large.parquet')\n",
    "count_per_year = tbl.group_by('year').aggregate([('year', 'count')])\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we been lucky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.8749954289989545 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "    count_per_year = df_large.groupby('year').size()\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.04289828349428717 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    tbl = pq.read_table('example_large.parquet')\n",
    "    count_per_year = tbl.group_by('year').aggregate([('year', 'count')])\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks promising. We understood a bit more about\n",
    "- the metadata of a Parquet file\n",
    "- how data is stored in row groups and column chunks\n",
    "- why is this relevant for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned the power of metadata, but is this metadata making the overall file larger?\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 18.72 MB\n",
      "File size: 139.39 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_size = os.path.getsize('example_large.parquet') / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.2f} MB\")\n",
    "\n",
    "\n",
    "file_size = os.path.getsize('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv') / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is actually an order-of-magnitude smaller. That's good news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "Next, let's use more features of writing Parquet files. The improvements we obtained so far were low-hanging fruits. We now loot at how partitioning works.\n",
    "\n",
    "We need to use the [write_dataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.write_dataset.html#pyarrow.dataset.write_dataset) function of PyArrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "arrow_table = pa.Table.from_pandas(df_large)\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "ds.write_dataset(\n",
    "    arrow_table,\n",
    "    \"./example_large_partitioned\",\n",
    "    format='parquet',\n",
    "    partitioning=ds.partitioning(pa.schema([(\"year\", pa.int32())]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2000\u001b[m\u001b[m\n",
      "\u001b[34m2001\u001b[m\u001b[m\n",
      "\u001b[34m2002\u001b[m\u001b[m\n",
      "\u001b[34m2003\u001b[m\u001b[m\n",
      "\u001b[34m2004\u001b[m\u001b[m\n",
      "\u001b[34m2005\u001b[m\u001b[m\n",
      "\u001b[34m2006\u001b[m\u001b[m\n",
      "\u001b[34m2007\u001b[m\u001b[m\n",
      "\u001b[34m2008\u001b[m\u001b[m\n",
      "\u001b[34m2009\u001b[m\u001b[m\n",
      "\u001b[34m2010\u001b[m\u001b[m\n",
      "\u001b[34m2011\u001b[m\u001b[m\n",
      "\u001b[34m2012\u001b[m\u001b[m\n",
      "\u001b[34m2013\u001b[m\u001b[m\n",
      "\u001b[34m2014\u001b[m\u001b[m\n",
      "\u001b[34m2015\u001b[m\u001b[m\n",
      "\u001b[34m2016\u001b[m\u001b[m\n",
      "\u001b[34m2017\u001b[m\u001b[m\n",
      "\u001b[34m2018\u001b[m\u001b[m\n",
      "\u001b[34m2019\u001b[m\u001b[m\n",
      "\u001b[34m2020\u001b[m\u001b[m\n",
      "\u001b[34m2021\u001b[m\u001b[m\n",
      "\u001b[34m2022\u001b[m\u001b[m\n",
      "\u001b[34m2023\u001b[m\u001b[m\n",
      "\u001b[34m2024\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls example_large_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-0.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls example_large_partitioned/2000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the [pyarrow.dataset.dataset()](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.dataset.html#pyarrow.dataset.dataset) function provides an interface to discover and read all those files as a single big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./example_large_partitioned/2000/part-0.parquet',\n",
       " './example_large_partitioned/2001/part-0.parquet',\n",
       " './example_large_partitioned/2002/part-0.parquet',\n",
       " './example_large_partitioned/2003/part-0.parquet',\n",
       " './example_large_partitioned/2004/part-0.parquet',\n",
       " './example_large_partitioned/2005/part-0.parquet',\n",
       " './example_large_partitioned/2006/part-0.parquet',\n",
       " './example_large_partitioned/2007/part-0.parquet',\n",
       " './example_large_partitioned/2008/part-0.parquet',\n",
       " './example_large_partitioned/2009/part-0.parquet',\n",
       " './example_large_partitioned/2010/part-0.parquet',\n",
       " './example_large_partitioned/2011/part-0.parquet',\n",
       " './example_large_partitioned/2012/part-0.parquet',\n",
       " './example_large_partitioned/2013/part-0.parquet',\n",
       " './example_large_partitioned/2014/part-0.parquet',\n",
       " './example_large_partitioned/2015/part-0.parquet',\n",
       " './example_large_partitioned/2016/part-0.parquet',\n",
       " './example_large_partitioned/2017/part-0.parquet',\n",
       " './example_large_partitioned/2018/part-0.parquet',\n",
       " './example_large_partitioned/2019/part-0.parquet',\n",
       " './example_large_partitioned/2020/part-0.parquet',\n",
       " './example_large_partitioned/2021/part-0.parquet',\n",
       " './example_large_partitioned/2022/part-0.parquet',\n",
       " './example_large_partitioned/2023/part-0.parquet',\n",
       " './example_large_partitioned/2024/part-0.parquet']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\"./example_large_partitioned\", format=\"parquet\", partitioning=ds.partitioning(pa.schema([(\"year\", pa.int32())])))\n",
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data has not been read yet. The whole dataset can be viewed as a single big table using [pyarrow.dataset.Dataset.to_table()](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html#pyarrow.dataset.Dataset.to_table).\n",
    "\n",
    "Notice that converting to a table will force all data to be loaded in memory. For big datasets is usually not what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "anzsic06: string\n",
      "Area: string\n",
      "geo_count: int64\n",
      "ec_count: int64\n",
      "year: int32\n",
      "----\n",
      "anzsic06: [[\"C25\",\"C25\",\"C25\",\"C25\",\"C25\",...,\"F341\",\"F341\",\"F341\",\"F341\",\"F341\"],[\"M70\",\"M70\",\"M70\",\"M70\",\"M70\",...,\"Q851\",\"Q851\",\"Q851\",\"Q851\",\"Q851\"],...,[\"O77\",\"O77\",\"O77\",\"O77\",\"O77\",...,\"R\",\"R\",\"R\",\"R\",\"R\"],[\"F373\",\"F373\",\"F373\",\"F373\",\"F373\",...,\"H440\",\"H440\",\"H440\",\"H440\",\"H440\"]]\n",
      "Area: [[\"A215000\",\"A215101\",\"A215200\",\"A215401\",\"A215600\",...,\"A119000\",\"A119500\",\"A119700\",\"A120000\",\"A120500\"],[\"A166400\",\"A166500\",\"A166700\",\"A167000\",\"A167101\",...,\"A146000\",\"A146400\",\"A146501\",\"A146600\",\"A146700\"],...,[\"A237100\",\"A237200\",\"A237500\",\"A237600\",\"A237700\",...,\"A161400\",\"A161500\",\"A161700\",\"A161800\",\"A161900\"],[\"R07\",\"R08\",\"R09\",\"R12\",\"R13\",...,\"A340700\",\"A341101\",\"A341201\",\"A341300\",\"A341400\"]]\n",
      "geo_count: [[0,0,0,3,3,...,0,0,0,0,3],[3,0,0,3,0,...,0,6,3,0,12],...,[0,0,3,0,3,...,15,6,3,6,0],[48,84,282,15,474,...,3,18,3,3,6]]\n",
      "ec_count: [[0,0,0,3,0,...,0,3,0,0,0],[15,0,0,0,3,...,0,6,0,0,18],...,[12,0,6,0,0,...,100,18,0,3,0],[200,230,930,9,1700,...,30,30,35,35,21]]\n",
      "year: [[2000,2000,2000,2000,2000,...,2000,2000,2000,2000,2000],[2000,2000,2000,2000,2000,...,2000,2000,2000,2000,2000],...,[2024,2024,2024,2024,2024,...,2024,2024,2024,2024,2024],[2024,2024,2024,2024,2024,...,2024,2024,2024,2024,2024]]\n"
     ]
    }
   ],
   "source": [
    "table = dataset.to_table()\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to exploit partitioning, the easiest is filtering when reading.\n",
    "\n",
    "Scan will return only the rows matching the filter. If possible the predicate will be pushed down to exploit the partition information or internal metadata found in the data source, e.g. Parquet statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "anzsic06: string\n",
      "Area: string\n",
      "geo_count: int64\n",
      "ec_count: int64\n",
      "year: int32\n",
      "----\n",
      "anzsic06: [[\"Total\",\"Total\",\"Total\",\"Total\",\"Total\",...,\"Total\",\"Total\",\"Total\",\"Total\",\"Total\"],[\"B08\",\"B08\",\"B08\",\"B08\",\"B08\",...,\"C259\",\"C259\",\"C259\",\"C259\",\"C259\"],...,[\"N73\",\"N73\",\"N73\",\"N73\",\"N73\",...,\"Q860\",\"Q860\",\"Q860\",\"Q860\",\"Q860\"],[\"Q860\",\"Q860\",\"Q860\",\"Q860\",\"Q860\",...,\"Total\",\"Total\",\"Total\",\"Total\",\"Total\"]]\n",
      "Area: [[\"A149600\",\"A149701\",\"A149702\",\"A149800\",\"A149901\",...,\"T073\",\"T074\",\"T075\",\"T076\",\"TTotal\"],[\"A335301\",\"A335701\",\"A336800\",\"A340700\",\"A343600\",...,\"A113300\",\"A113402\",\"A113800\",\"A114000\",\"A114500\"],...,[\"A330600\",\"A330700\",\"A330800\",\"A330900\",\"A331000\",...,\"A305600\",\"A305800\",\"A305900\",\"A306100\",\"A306501\"],[\"A306801\",\"A307301\",\"A307501\",\"A307601\",\"A307801\",...,\"A149100\",\"A149200\",\"A149300\",\"A149400\",\"A149500\"]]\n",
      "geo_count: [[153,270,114,477,450,...,6960,2049,5316,207252,596973],[3,0,0,0,3,...,0,3,6,0,6],...,[3,3,3,3,6,...,0,3,3,0,0],[3,0,6,3,0,...,168,300,210,111,81]]\n",
      "ec_count: [[2200,390,80,1350,330,...,16500,6600,28000,813900,2332600],[9,0,0,0,560,...,0,0,3,0,6],...,[0,12,3,6,0,...,40,220,310,0,18],[18,0,210,70,30,...,510,400,570,290,190]]\n",
      "year: [[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],...,[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020],[2020,2020,2020,2020,2020,...,2020,2020,2020,2020,2020]]\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(ds.field('year') == 2020)\n",
    "table = filtered_dataset.to_table()\n",
    "print(table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
