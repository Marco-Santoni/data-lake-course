{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parquet file\n",
    "\n",
    "How can we create a Parquet file in Python?\n",
    "\n",
    "Let's start from a Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'one': [-1, 0, 2.5],\n",
    "        'two': ['foo', 'bar', 'baz'],\n",
    "        'three': [True, False, True]\n",
    "    },\n",
    "    index=list('abc')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Apache Arrow _specification_.\n",
    "\n",
    "\n",
    "> Apache Arrow was born from the need for a **set of standards** around tabular data representation and interchange between systems. The adoption of these standards reduces computing costs of data serialization/deserialization and implementation costs across systems implemented in different programming languages.\n",
    "\n",
    "In Python, we can use PyArrow, the Python implementation of the Arrow specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pyarrow.Table object\n",
    "\"\"\"The PyArrow Table type is not part of the Apache Arrow specification, but is rather a tool to help with wrangling multiple record batches and array pieces as a single logical dataset. As a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\"\"\"\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table\n",
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet metadata..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x12f8c4220>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 4\n",
       "  num_rows: 3\n",
       "  num_row_groups: 1\n",
       "  format_version: 2.6\n",
       "  serialized_size: 2572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata('example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is metadata?\n",
    "https://parquet.apache.org/docs/file-format/metadata/\n",
    "We have just read the `FileMetadata`\n",
    "\n",
    "> The file metadata is described by the `FileMetaData` structure. This file metadata provides offset and size information useful when navigating the Parquet file. \n",
    "\n",
    "![Parquet Metadata](docs/FileFormat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger file\n",
    "\n",
    "https://www.stats.govt.nz/large-datasets/csv-files-for-download/\n",
    "https://www.stats.govt.nz/assets/Uploads/New-Zealand-business-demography-statistics/New-Zealand-business-demography-statistics-At-February-2024/Download-data/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anzsic06     Area  year  geo_count  ec_count\n",
      "0        A  A100100  2024         87       200\n",
      "1        A  A100200  2024        135       210\n",
      "2        A  A100301  2024          6        35\n",
      "3        A  A100400  2024         54        35\n",
      "4        A  A100500  2024         51        95\n",
      "(6751326, 5)\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.head())\n",
    "print(df_large.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write again this table to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(df_large), 'example_large.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig more into the Metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x13500f6f0>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 6751326\n",
       "  num_row_groups: 6752\n",
       "  format_version: 2.6\n",
       "  serialized_size: 3260180"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Parquet 100% columnar?\n",
    "\n",
    "What are row groups?\n",
    "Ref:\n",
    "https://blog.det.life/i-spent-8-hours-learning-parquet-heres-what-i-discovered-97add13fb28f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional row-wise formats store data as records, one after another, much like a database table. This format is intuitive and works well when accessing entire records frequently. However, it can be inefficient when dealing with analytics, where you often only need specific columns from a large dataset.\n",
    "\n",
    "![row storage](./docs/row-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar formats address this issue by storing data in columns instead of rows. This means that when you need specific columns, you can read only the data you need, significantly reducing the amount of data scanned.\n",
    "\n",
    "![columnar storage](./docs/columnar-storage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, simply storing data in a columnar format has some downsides. The record **write or update** operation requires touching **multiple column segments**, resulting in numerous **I/O** operations. This can significantly slow the write performance, especially when dealing with large datasets.\n",
    "\n",
    "In addition, when queries involve multiple columns, the database system must reconstruct the records from separate columns. The cost of this reconstruction increases with the **number of columns** involved in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![row groups](./docs/row-groups.webp)\n",
    "\n",
    "The format groups data into “row groups,” each containing a subset of rows. (horizontal partition.) Within each row group, data for each column is called a “column chunk.” (vertical partition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page header\n",
    "\n",
    "We will not cover a further level of granular metadata: `PageHeader`\n",
    "\n",
    "The page header metadata is stored with the page data and includes information such as value encoding, definition encoding, and repetition encoding. In addition to the data values, Parquet also stores definition and repetition levels to handle nested data. The application uses the page header to **read and decode** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.RowGroupMetaData object at 0x135078630>\n",
       "  num_columns: 5\n",
       "  num_rows: 1000\n",
       "  total_byte_size: 15617\n",
       "  sorting_columns: ()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's look att Row Group metadata\n",
    "\n",
    "parquet_file.metadata.row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ColumnChunkMetaData object at 0x1135e53f0>\n",
       "  file_offset: 0\n",
       "  file_path: \n",
       "  physical_type: INT64\n",
       "  num_values: 1000\n",
       "  path_in_schema: year\n",
       "  is_stats_set: True\n",
       "  statistics:\n",
       "    <pyarrow._parquet.Statistics object at 0x13500f790>\n",
       "      has_min_max: True\n",
       "      min: 2024\n",
       "      max: 2024\n",
       "      null_count: 0\n",
       "      distinct_count: None\n",
       "      num_values: 1000\n",
       "      physical_type: INT64\n",
       "      logical_type: None\n",
       "      converted_type (legacy): NONE\n",
       "  compression: SNAPPY\n",
       "  encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
       "  has_dictionary_page: True\n",
       "  dictionary_page_offset: 5607\n",
       "  data_page_offset: 5631\n",
       "  total_compressed_size: 99\n",
       "  total_uncompressed_size: 95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file.metadata.row_group(0).column(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this statistic useful?\n",
    "Let's try a basic count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anzsic06     6751326\n",
      "Area         6751326\n",
      "year         6751326\n",
      "geo_count    6751326\n",
      "ec_count     6751326\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 6751326\n"
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "row_count = parquet_file.metadata.num_rows\n",
    "print(f\"Total number of rows: {row_count}\")\n",
    "#given this pyarrow parquet file, compute the count of rows in the year column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something more sophisticated, count the rows per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2000    247192\n",
      "2001    245753\n",
      "2002    246695\n",
      "2003    250130\n",
      "2004    259121\n",
      "2005    263223\n",
      "2006    265095\n",
      "2007    266518\n",
      "2008    267211\n",
      "2009    268547\n",
      "2010    267549\n",
      "2011    267954\n",
      "2012    268210\n",
      "2013    267911\n",
      "2014    270514\n",
      "2015    273210\n",
      "2016    275177\n",
      "2017    277515\n",
      "2018    278822\n",
      "2019    281313\n",
      "2020    282621\n",
      "2021    283621\n",
      "2022    290242\n",
      "2023    293312\n",
      "2024    293870\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "count_per_year = df_large.groupby('year').size()\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we avoid using Pandas, but just PyArrow engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "year: int64\n",
      "year_count: int64\n",
      "----\n",
      "year: [[2024,2023,2022,2021,2020,...,2004,2003,2002,2001,2000]]\n",
      "year_count: [[293870,293312,290242,283621,282621,...,259121,250130,246695,245753,247192]]\n"
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "dataset = ds.dataset('example_large.parquet', format='parquet', )\n",
    "year_column = dataset.to_table(columns=['year'])\n",
    "count_per_year = year_column.group_by('year').aggregate([('year', 'count')])\n",
    "print(count_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we been lucky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.8749954289989545 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "    count_per_year = df_large.groupby('year').size()\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.23891639159992337 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "import timeit\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(10):\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Your code\n",
    "    parquet_file = pq.ParquetFile('example_large.parquet')\n",
    "    dataset = ds.dataset('example_large.parquet', format='parquet', )\n",
    "    year_column = dataset.to_table(columns=['year'])\n",
    "    count_per_year = year_column.group_by('year').aggregate([('year', 'count')])\n",
    "\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "average_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Average execution time: {average_execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x1077d8f90>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 5\n",
       "  num_rows: 6751326\n",
       "  num_row_groups: 6752\n",
       "  format_version: 2.6\n",
       "  serialized_size: 3260180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata('example_large.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref row group\n",
    "https://blog.det.life/i-spent-8-hours-learning-parquet-heres-what-i-discovered-97add13fb28f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine 6M records is large, we can partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011\n",
      " 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000]\n"
     ]
    }
   ],
   "source": [
    "distinct_years = df_large['year'].unique()\n",
    "print(distinct_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(\n",
    "    pa.Table.from_pandas(df_large),\n",
    "    'example_large.parquet',\n",
    "    ProgressBar\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
