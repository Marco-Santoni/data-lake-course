{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parquet file\n",
    "\n",
    "How can we create a Parquet file in Python?\n",
    "\n",
    "Let's start from a Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'one': [-1, 0, 2.5],\n",
    "        'two': ['foo', 'bar', 'baz'],\n",
    "        'three': [True, False, True]\n",
    "    },\n",
    "    index=list('abc')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Apache Arrow _specification_.\n",
    "\n",
    "\n",
    "> Apache Arrow was born from the need for a **set of standards** around tabular data representation and interchange between systems. The adoption of these standards reduces computing costs of data serialization/deserialization and implementation costs across systems implemented in different programming languages.\n",
    "\n",
    "In Python, we can use PyArrow, the Python implementation of the Arrow specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pyarrow.Table object\n",
    "\"\"\"The PyArrow Table type is not part of the Apache Arrow specification, but is rather a tool to help with wrangling multiple record batches and array pieces as a single logical dataset. As a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\"\"\"\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table\n",
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet metadata..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x12f8c4220>\n",
       "  created_by: parquet-cpp-arrow version 18.0.0\n",
       "  num_columns: 4\n",
       "  num_rows: 3\n",
       "  num_row_groups: 1\n",
       "  format_version: 2.6\n",
       "  serialized_size: 2572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata('example.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is metadata?\n",
    "https://parquet.apache.org/docs/file-format/metadata/\n",
    "We have just read the `FileMetadata`\n",
    "\n",
    "> The file metadata is described by the `FileMetaData` structure. This file metadata provides offset and size information useful when navigating the Parquet file. \n",
    "\n",
    "![Parquet Metadata](docs/FileFormat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anzsic06     Area  year  geo_count  ec_count\n",
      "0        A  A100100  2024         87       200\n",
      "1        A  A100200  2024        135       210\n",
      "2        A  A100301  2024          6        35\n",
      "3        A  A100400  2024         54        35\n",
      "4        A  A100500  2024         51        95\n",
      "(6751326, 5)\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv('./geographic-units-by-industry-and-statistical-area-2000-2024-descending-order/geographic-units-by-industry-and-statistical-area-2000-2024-descending-order-february-2024.csv')\n",
    "print(df_large.head())\n",
    "print(df_large.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine 6M records is large, we can partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011\n",
      " 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000]\n"
     ]
    }
   ],
   "source": [
    "distinct_years = df_large['year'].unique()\n",
    "print(distinct_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(\n",
    "    pa.Table.from_pandas(df_large),\n",
    "    'example_large.parquet',\n",
    "    ProgressBar\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_file_formats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
